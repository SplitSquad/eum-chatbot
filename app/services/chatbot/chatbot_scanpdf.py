import requests
import fitz
import os
import pdfplumber
from langchain_groq import ChatGroq
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
import json
from dotenv import load_dotenv
load_dotenv()  # .env íŒŒì¼ ìë™ ë¡œë”©
import easyocr
import glob  # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°ìš©

############################################# pdf í…ìŠ¤íŠ¸ ì¶”ì¶œ
def text_extraction(PDF_FILE_PATH):
    
    # âœ… ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
    text_list = []
    with pdfplumber.open(PDF_FILE_PATH) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                text_list.append(text.strip())

    # âœ… ì¶œë ¥ ì˜ˆì‹œ
    for i, page_text in enumerate(text_list):
        print(f"\nğŸ“„ Page {i+1} í…ìŠ¤íŠ¸:\n{page_text}")

    # âœ… ì €ì¥í•  í´ë” ìƒì„±
    os.makedirs('./data', exist_ok=True)

    # âœ… PDF â†’ PNG ë³€í™˜
    doc = fitz.open(PDF_FILE_PATH)
    for i, page in enumerate(doc):
        img = page.get_pixmap()
        output_path = f"./data/{i}.png"
        img.save(output_path)
        # print(f"ğŸ“¸ ì €ì¥ ì™„ë£Œ: {output_path}")

    ###### first_ocr
    image_files = sorted(glob.glob('./data/*.png')) # âœ… ëª¨ë“  PNG íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ì •ë ¬ í¬í•¨)
    first_ocr_text_list = []         # âœ… ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    # âœ… ì‚¬ìš©ì ì„¤ì •
    api_key = 'K85417667888957'  # ë°œê¸‰ë°›ì€ API í‚¤
    language = 'eng'              # 'kor'ë„ ê°€ëŠ¥
    overlay = False               # ë‹¨ì–´ ìœ„ì¹˜ ì¢Œí‘œ í•„ìš” ì—¬ë¶€
    # âœ… payload êµ¬ì„±
    payload = {
        'isOverlayRequired': overlay,
        'apikey': api_key,
        'language': language,
    }
    # âœ… OCR API ë°˜ë³µ í˜¸ì¶œ
    for filename in image_files:
        with open(filename, 'rb') as f:
            response = requests.post(
                'https://api.ocr.space/parse/image',
                files={'filename': f},
                data=payload,
            )
        result = response.json()
        parsed_text = result['ParsedResults'][0]['ParsedText']
        first_ocr_text_list.append(parsed_text.strip())
    # âœ… ê²°ê³¼ ì¶œë ¥
    for i, text in enumerate(first_ocr_text_list):  
        print(f"\nğŸ“¸ first OCR ê²°ê³¼ Page {i+1}:\n{text}")
    first_ocr_text = "------".join(first_ocr_text_list) 

    ###### second_ocr
    reader = easyocr.Reader(['ko'])  # í•œêµ­ì–´ + ì˜ì–´, CPU ëª¨ë“œ
    second_ocr_lines = []
    for filename in image_files:
        result = reader.readtext(filename)
        for bbox, text, confidence in result:
            second_ocr_lines.append(f"[{confidence:.2f}] {text}")
    # ì¤„ë°”ê¿ˆ í¬í•¨í•˜ì—¬ ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
    second_ocr_text = "------".join(second_ocr_lines)

    ###### third_ocr


    # âœ… PDFì—ì„œ ì¶”ì¶œëœ text_listì™€ ê²°í•©
    parsed_text = (
        "ğŸ“„Text extracted from pdf \n"
        + "\n---\n".join(text_list)
        + "\n\n\n\n ğŸ“¸Text from image extracted from pdf \n"
        + " first OCR MODEL \n"
        + first_ocr_text
        + " second OCR MODEL \n"
        + second_ocr_text
    )

    print("parsed_text",parsed_text)

    # âœ… OCR ì´í›„, data í´ë” ë‚´ PNG ì´ë¯¸ì§€ ì‚­ì œ
    for file in glob.glob('./data/*.png'):
        os.remove(file)

    return parsed_text
############################################# pdf í…ìŠ¤íŠ¸ ì¶”ì¶œ


############################################# pdf ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
def classification(parsed_text) :
    llm = ChatGroq(
        model_name="llama-3.3-70b-versatile",
        temperature=0.7
    )

    parser = JsonOutputParser(pydantic_object={
        "type": "object",
        "properties": {
            "input": {"type": "string"},
            "output": {"type": "string"},
        }
    })

    prompt = ChatPromptTemplate.from_messages([
    ("system", """
    1. Your role is to categorize the PDFs sent by users.
    2. Return only a JSON object with the following format:
    "input": "...",
    "output": "..."  // One of: agentic-calendar, agentic-resume, agentic-create, agentic-visa, general

    
    3. Do not add any explanations. Do not return markdown. Do not add commentary.

    input : "2025ë…„ 3ì›” 4ì¼ ~ 10ì¼: ì œ1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ë³€ê²½ ê¸°ê°„"
    output : "agentic-calendar"

    input : "2025ë…„ 6ì›” 30ì¼: ì œ1í•™ê¸° ì„±ì ì œì¶œ ë§ˆê°ì¼"
    output : "agentic-calendar"
    
    input : "2026ë…„ 1ì›” 6ì¼ ~ 10ì¼: 2026í•™ë…„ë„ ì „ê³¼ ì‹ ì²­"
    output : "agentic-calendar"

    input : "ì œ2í•™ê¸° ê°œê°•: 2025ë…„ 9ì›” 1ì¼"
    output : "agentic-calendar"

    input : "ì œ2í•™ê¸° ì¬í•™ìƒ ë“±ë¡: 8ì›” 25ì¼ ~ 29ì¼"
    output : "agentic-calendar"

    input : "2026ë…„ 2ì›” 28ì¼: ë™ê³„íœ´ê°€ ì¢…ë£Œ"
    output : "agentic-calendar"

    input : "ì œ1í•™ê¸° ë³´ê°• ê¸°ê°„: 6ì›” 17ì¼ ~ 23ì¼"
    output : "agentic-calendar"

    


    input : "ì´ë ¥ì„œ"
    output : "agentic-resume"

    input : "ì„±ëª…: í™ê¸¸ë™ / ìƒë…„ì›”ì¼: 1990.01.01"
    output : "agentic-resume"

    input : "í•™ë ¥: 2010~2014 ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ì¡¸ì—…"
    output : "agentic-resume"

    input : "ê²½ë ¥: 2015~2020 Naver Corp. ë°±ì—”ë“œ ê°œë°œì"
    output : "agentic-resume"

    input : "ìê²©ì¦: ì •ë³´ì²˜ë¦¬ê¸°ì‚¬"
    output : "agentic-resume"

    input : "ìê¸°ì†Œê°œì„œ"
    output : "agentic-resume"

    input : "í•™ìœ„: ì„ì‚¬ / ì „ê³µ: ì¸ê³µì§€ëŠ¥"
    output : "agentic-resume"

    input : "ê²½ë ¥ì‚¬í•­ ìš”ì•½"
    output : "agentic-resume"


    

    input : "2024 ëŒ€ì „ ì´ì‹œ ì¶•ì œ ê°œìµœ ì•ˆë‚´"
    output : "agentic-create"

    input : "í–‰ì‚¬ì¼ì‹œ: 2024ë…„ 8ì›” 9ì¼ ~ 17ì¼"
    output : "agentic-create"

    input : "K-pop ì½˜ì„œíŠ¸, ê³¼í•™ ì „ì‹œ ì²´í—˜, í¼ë ˆì´ë“œ"
    output : "agentic-create"

    input : "ë°”ë¡œê°€ê¸° / ëŒ€ì „ì‹œ ê³µì‹ í–‰ì‚¬"
    output : "agentic-create"

    input : "í–‰ì‚¬ í¬ìŠ¤í„° / ì¥ì†Œ: ì˜› ì¶©ë‚¨ë„ì²­"
    output : "agentic-create"

    input : "ì¶œì—°ì§„: í™”ì‚¬, ì œì‹œ, ë‹¤ë¹„ì¹˜ ë“±"
    output : "agentic-create"

    input : "ê³¼í•™ê¸°ìˆ  í…Œë§ˆíŒŒí¬ / ì²´í—˜ ë¶€ìŠ¤ ìš´ì˜"
    output : "agentic-create"

    input : "ì´ë¬´ì§„, ë°•ì§€í˜„, ì¥ë¯¼í˜¸ ë“± ì¶œì—°"
    output : "agentic-create"

    

    input : "IMMIGRANT VISA / IV Case Number: 00200416000201"
    output : "agentic-visa"

    input : "Given name: HAPPY / Nationality: GBR"
    output : "agentic-visa"

    input : "Issue Date: 23DEC2004 / Passport Number: 555123ABC"
    output : "agentic-visa"

    input : "US CONSULATE GENERAL - LONDON"
    output : "agentic-visa"

    input : "Visa Type: TRAVELER"
    output : "agentic-visa"

    input : "Date of Birth: 05FEB1965 / Status: IV On"
    output : "agentic-visa"

    input : "SERVES I-551 / Immigrant Visa Status"
    output : "agentic-visa"

    input : "Case No: 00000473 / Category: F11"
    output : "agentic-visa"

    input : "Immigration Barcode: 555123ABC6GBR6502056F04122361FLNDOOAMS803085"
    output : "agentic-visa"

    input : "Residence: United Kingdom / Entry Code: IV"
    output : "agentic-visa"

    

    intput : All answers other than the above examples
    output : general


    
    Only return the JSON object like above.
    """
    
    ),
        ("user", "{input}")
    ])

    chain = prompt | llm | parser

    def parse_product(description: str) -> dict:
            result = chain.invoke({"input": description})
            print(json.dumps(result, indent=2, ensure_ascii=False))

            return json.dumps(result, indent=2, ensure_ascii=False)

    description = parsed_text
    print("description\n",description)
    response = parse_product(description)
    response = json.loads(response)
    print("response\n",response)
    
    return response

############################################# pdf ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜


############################################# pdf ìš”ì•½ 
def pdf_general(parsed_text) :

    llm = ChatGroq(
        model_name="llama-3.3-70b-versatile",
        temperature=0.7
        )

    parser = JsonOutputParser(pydantic_object={
        "type": "object",
        "properties": {
            "input": {"type": "string"},
            "output": {"type": "string"},
        }
    })
    prompt = ChatPromptTemplate.from_messages([
    ("system", """
    1. You are the part that organizes the text.
    2. I will give you the text extracted from the pdf.
    3. Please organize the text so that it is easy for users to understand.

    Return only this JSON:
    
    {{ 
    "input": "Original text",
    "output": "organized text"
    }}
    """
    
    ),
        ("user", "{input}")
    ])

    chain = prompt | llm | parser

    def parse_product(description: str) -> dict:
        result = chain.invoke({"input": description})
        print(json.dumps(result, indent=2, ensure_ascii=False))

        return json.dumps(result, indent=2, ensure_ascii=False)

    description = parsed_text
    print("description\n",description)
    response = parse_product(description)
    print("response\n",response)

    return response
############################################# pdf ìš”ì•½

############################################# ë©”ì¸
# âœ… ì˜ˆì œ PDF íŒŒì¼ ê²½ë¡œ
PDF_FILE_PATH = "./í•™ì‚¬ì¼ì •.pdf"
parsed_text = text_extraction(PDF_FILE_PATH) # pdf ì¶”ì¶œ
Category = classification(parsed_text) #
Category = Category["output"]

if Category == "general" :
    print("general")
    general=pdf_general(parsed_text)
    print("general\n",general)
elif Category == "agentic-calendar" :
    print("agentic-calendarë¡œ ë¶„ë¥˜ë¨")
elif Category == "agentic-create" :
    print("agentic-createë¡œ ë¶„ë¥˜ë¨")
elif Category == "agentic-visa" :
    print("agentic-visaë¡œ ë¶„ë¥˜ë¨")
elif Category == "agentic-resume" :
    print("agentic-resumeë¡œ ë¶„ë¥˜ë¨")
else :
    print("ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬ ì…ë‹ˆë‹¤.")

print("Category\n",Category)